# Validation set
*Why need good validation set?*
There are common cases that the model complete fails in the production.

*what is validation set?*
- Training set is used to train different models
- Validation set is used to distinguish/choose models
- Test set is to measure how your best final model works

Validation set and test set has common property: **representative to the data you will see** (in production/real world).

Validation set usually randomly selected, but might not work in some scenarios:
- Time series
- Data in production is *qualitatively different*

## Size of validation set
*How large the validation set should be?*
This problem should be rephrased as: **how precisely need  to know the accuracy of the algorithm/model?**

1. There is a suggestion generally should be **higher than 22**. At least 22 in a class in any set(training/evaluation/test). 22 is the magic number where the t-distribution roughly turns into normal distribution.

    *What if the sample for certain class is too small?*
    - The validation set, test set need to have the same mix or frequency of the observations that you will see in the prod.
    - Training set should have equal number in each class and if not just replicate the less common one until it is equal.

2. Another approach to figure out whether it is big enough:
- Train model couple times(say 5) with same hyper parameter each time, and see the validation set accuracy. The mean and a standard deviation of 5 numbers (or a maximum/minimum) can use. 

Example: if you have .99 accuracy, then the standard deviation is 0.99*0.01, then you could calculate the standard error: `n*p*(1-p)/sqrt(n)`, i.e. `sqrt(n)*p*(1-p)`. Hence you could know how large the n is.

## Some mathmatical calculation
In binomial distribution of n samples and probability p:
- The **mean**: `n*p`
- The **standard deviation**: `n*p*(1-p)`

*Why we need to the stardand deviation here?* 

**Standard error** is if you run a bunch of trials, each time getting a mean, what is the standard deviation of the mean.

If you train a hundred models, each time the validation set accuracy is like the mean of a distribution.

The standard deviation of that the validation set accuracy, can be calculated with the standard error and it equals to the standard deviation devided by square root n.

# 2. Cross Validation
Split the randomly shuffle data into N splits, and take N-1 splits to train and the left one to evaluate, then everage the final score.

Please note that cross-validation solution is rarely used in real word problems.

## Cross-validation vs. standard validation set
The benefit is it could be able to use all the data.

## What scenarios should not use cross-validation?
1. Large dataset which take a long time to train
2. Random validation set is not appropriate for the problem, like temporal data


# 3. Tools

## Waterfall plots
> Every time you have a starting point and a number of changes and a finishing point, waterfall charts are pretty much always the best way to show it.


## Referenece
- https://www.fast.ai/2017/11/13/validation-sets/
- https://medium.com/@hiromi_suenaga/machine-learning-1-lesson-5-df45f0c99618
- https://medium.com/@hiromi_suenaga/machine-learning-1-lesson-7-69c50bc5e9af
