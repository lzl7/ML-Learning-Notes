# What is random forest
Random forest consists of a bunch of tree in bagging approach.

A tree consists of a sequence of binary decisions. Why binary is enough? Multiple dicsions could be represented by a sequence of binary decisions.

# How to create a tree?
To create tree is actually about creating binary decision. How to do that?
- Which variable? And what is the value to pick? 
  - It needs to pick a *variable* and the *value* to split on such that the two groups are as different to each other as possible
  - How to pick? (Or what value should be picked)? For each variable, for each possible value of that variable see whether it is better
  - How to determine if it is better? Take weighted average of the mean squared errors of the two groups.
  - When to stop the split? Several stop conditions:
    - When it hits the requested limit (like `max_depth`)
    - When the leaf nodes only have one item

# How the bagging works?

Within the forest, it creates a bunch of trees which create a different model that profound different insights into the data.

Each tree only take proportion of the data for training. There are several ways to create the training set for each tree, basically is the random(exclusive) and bootstrapping methods. Usually, it uses the bootstrapping method in random forest.  

The research shows that it is more important to create *uncorrelated trees* rather than more *accurate trees*.

*How to leverage the predictions from different models?* **Ensembling**.


# OOB (out-of-bag) score
## Why
In some cases, you only have small dataset and might not be a good idea to create a validation set (otherwise you don't have enough data to build a good model). The out-of-bag error could help tackle this scenario.

## How
The basic idea is used those (part of) data which not used for training in the current tree as the validation set, then run the RMSE, R2, etc. And then average all the validation scores as the OOB error to measure the quanlity.

# Hyper parameters
Grid search, which runs the model on every possible combination of all these hyper parameters and tell you which one is the best, is the approach to automatically find/tune the hyper parameters. Scikit-learn has a function call grid search.

OOB score is a good choice used in the grid search to tell which one is the best.