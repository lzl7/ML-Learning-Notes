# What is random forest
Random forest consists of a bunch of tree in bagging approach.

A tree consists of a sequence of binary decisions. Why binary is enough? Multiple dicsions could be represented by a sequence of binary decisions.

# How to create a tree?
To create tree is actually about creating binary decision. How to do that?
- Which variable? And what is the value to pick? 
  - It needs to pick a *variable* and the *value* to split on such that the two groups are as different to each other as possible
  - How to pick? (Or what value should be picked)? For each variable, for each possible value of that variable see whether it is better
  - How to determine if it is better? Take weighted average of the mean squared errors of the two groups.
  - When to stop the split? Several stop conditions:
    - When it hits the requested limit (like `max_depth`)
    - When the leaf nodes only have one item

# How the bagging works?

Within the forest, it creates a bunch of trees which create a different model that profound different insights into the data.

Each tree only take proportion of the data for training. There are several ways to create the training set for each tree, basically is the random(exclusive) and bootstrapping methods. Usually, it uses the bootstrapping method in random forest.  

The research shows that it is more important to create *uncorrelated trees* rather than more *accurate trees*.

*How to leverage the predictions from different models?* **Ensembling**.