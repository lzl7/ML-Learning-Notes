# 1. What is random forest
Random forest consists of a bunch of tree in bagging (boostrap aggregation) approach.

A tree consists of a sequence of binary decisions. Why binary is enough? Multiple dicsions could be represented by a sequence of binary decisions.

# 2. How to create a tree?
To create tree is actually about creating binary decision. How to do that?
- Which variable? And what is the value to pick? 
  - It needs to pick a *variable* and the *value* to split on such that the two groups are as different to each other as possible
  - How to pick? (Or what value should be picked)? For each variable, for each possible value of that variable see whether it is better
  - How to determine if it is better? **Information** is the term used to measure how much we gain from a split (i.e. the amount of difference). The information gain means how much better the modele got by adding an additional split points. The measurement could based on RMSE, cross-entropy or how different to standard deviations, etc. For example: take weighted average of the mean squared errors of the two groups.
  - When to stop the split? Several stop conditions:
    - When it hits the requested limit (like `max_depth`)
    - When the leaf nodes only have one item

## Number of (min) samples in the leaf
With more samples in a leaf, we have less depth and a smaller number of leaf nodes. Hense, each estimator would be less predictive, but estimators would be also less correlated. It might help avoid overfitting. However, we need to have a trade-off and balance, too much samples in a leaf would lead to weak prediction.

## Number/Percentage of (max) columns of feature
This way of thinking is to randomly pick a portion of features in each split. In this way, the tree could be as rich as possible. Especially when you have a small number of estimators. 

The effect of the `max_feature` is the same as the `min_samples_leaf`: each individual estimator is more likely be less accurate but the estimators are going to be more varied.

## Relationship #estimators and accuracy
As increasing the #estimators, it will improve the accuracy at the beginning and then not moving much, since they are pretty the same. The reason they are almost the same is because they all use all the features (i.e. all variables). If you set the `max_features=sqrt` or log, when adding more estimators, it might be continuously improving since the features are randomly picked.

## Suggestion on the parameters
Generally, these setting might help:
- `max_features`: None, 0.5, sqrt, log2
- `min_samples_leaf`:1,3,5,10,25,100...

# 3. How the bagging works?

Within the forest, it creates a bunch of trees which create a different model that profound different insights into the data.

Each tree only take proportion of the data for training. There are several ways to create the training set for each tree, basically is the random(exclusive) and bootstraping methods. The boostrap approach is randomly sampled-with-replacement way to generate the subset of the data. Usually, it uses the bootstrapping method in random forest.  

The research shows that it is more important to create *uncorrelated trees* rather than more *accurate trees*.

*How to leverage the predictions from different models?* **Ensembling**.

## Suggestions
When building model wiht bagging, we should try to do two things:
- Each individual tree/estimator should be as accurate as possible (i.e. as strong predictive model)
- Across estimators, correlation is as low as possible, which could be it generalize enough

# What impacts the training time for random forest?
- The number of trees multiplied the number of data size used in training.

# 4. OOB (out-of-bag) score
## Why
In some cases, you only have small dataset and might not be a good idea to create a validation set (otherwise you don't have enough data to build a good model). The out-of-bag error could help tackle this scenario.

OOB only is only output for training, it does not impact the accurate of the model. But developer could based on the OOB feedback to continue improving the model's accuracy.

## How
The basic idea is used those (part of) data which not used for training in the current tree as the validation set, then run the RMSE, R2, etc. And then average all the validation scores as the OOB error to measure the quanlity.

# Hyper parameters
Grid search, which runs the model on every possible combination of all these hyper parameters and tell you which one is the best, is the approach to automatically find/tune the hyper parameters. Scikit-learn has a function call grid search.

OOB score is a good choice used in the grid search to tell which one is the best.

## Comparing with validation score
1. Validation set is a separate data set from the training data set, while the OOB only uses the data items not in the current model training set.
2. Validation score is calculated using all the decision trees of the ensemble (i.e. random forest), while OOB score is calculated using a subset of the decision trees which does not contain the OOB sample in their training set.
3. In general, the validation score is better than OOB since it leverages all the decision trees to measure.

## Time serial data
With the time serial data, the OOB score mihgt be better than the validation score, since the OOB samples are random while the validation set are in different time perid.

## How much data should be used for OOB sample?
The percentage is ~36.8%. With the sampling-with-replacement, the probability of not picking N rows is (N-1/N) ^ N, which in the limit of N to be infinite and it equals to e^-1 = 0.368.

***Sampling-with-replacement vs sampling w/o replacement***
Sampling with replacment means that each sampling, the data set's samples will not change, i.e. the selected sample will replace back to the pool for selecting. Without replacement does the opposite way, it will not replace it back for selecting again.

In with-replacement approach, a sample could be selected more than once while the without-replacement can only select once.

With-replacement is independent while without-replacment is dependent.

# 5. How to evaluation
Usually, you will have several levels of testing (and you might want to move to the next only when the previous level looks good):
1. Test on OOB
2. Test on validation set
3. Test on test set


# Reference
- https://course18.fast.ai/ml
- https://towardsdatascience.com/what-is-out-of-bag-oob-score-in-random-forest-a7fa23d710
- https://web.ma.utexas.edu/users/parker/sampling/repl.htm
- https://medium.com/@hiromi_suenaga/machine-learning-1-lesson-4-a536f333b20d
